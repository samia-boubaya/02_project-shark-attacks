{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856a10e3",
   "metadata": {},
   "source": [
    "# PROJECT 02 :\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b01db6",
   "metadata": {},
   "source": [
    "### BRIEF : data analysis of global shark attacks for a business idea\n",
    "You will initially examine the Shark Attack dataset, understanding its structure and formulating a hypothesis or several hypotheses about the data. \n",
    " We hypothesize that shark attacks are more common in certain locations and peak during specific months.\n",
    "We define a Business Case, such as \n",
    "\n",
    "* ‚ÄúAs a company that sells medical products, I want to identify destinations with high shark attack rates.‚Äù\n",
    "* ‚ÄúAs a company providing supply transportation services, I want to know when and where shark attacks peak to plan the safe transport of medical supplies to hospitals.‚Äù\n",
    "\n",
    " Throughout the project, we will use Python and the pandas library to apply at least five data cleaning techniques to handle missing values, duplicates, and formatting inconsistencies. After cleaning, we will perform basic exploratory data analysis to validate our hypotheses and extract insights. \n",
    "\n",
    "üìù BUSINESS IDEA ‚Äî 3 Bullet Points\n",
    "\n",
    "* Problem to Solve: Coastal hospitals and emergency response teams are not always prepared with the right medical supplies during periods of high shark-attack frequency. This business solves the problem by predicting when and where attacks are most likely, so medical supplies can be stocked in advance.\n",
    "\n",
    "* Business Concept: Use historical shark attack data to create global heatmaps and seasonal risk forecasts. Then provide pharmaceutical products (painkillers, antibiotics, blood bags, emergency kits) and transportation support to hospitals and ambulances near high-risk beaches.\n",
    "\n",
    "* Data Used to Profit: The business will analyze Country, Date/Month, Gender, Age, Fatality, and Type of Injury to identify high-risk locations, peak attack months, and most common injury types. This allows optimized supply production, targeted sales, and efficient delivery to the areas that need it most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5189c7",
   "metadata": {},
   "source": [
    "## üåÄCOLUMNS TO CLEAN : \n",
    "-----\n",
    "\n",
    "**- COUNTRY (global comparison between countries to invest in more)**@Blanca\n",
    "\n",
    "    * we just take the country column and make sure every country name is accurately named\n",
    "    * We are gonna check the column of COUNTRY and make sure every country name is correct\n",
    "    * We remove NULL COUNTRY data rows that dont have any country\n",
    "    * We need to make sure that the name of COUNTRY  is capitalized and written the same way for example :\n",
    "        United States of Amercia == USA == US\n",
    "        it has to have the same name and consistent!\n",
    "\n",
    "**- DATE ( MONTH + YEAR )**@Blanca\n",
    "\n",
    "    * split the date into day - month columns and only use month column\n",
    "    * interpret months which have shark attacks happen the most\n",
    "\n",
    "**- GENDER ( F or M )** @Cecilia\n",
    "\n",
    "    * We check unique values and make it so it is only two values F or M and deleted all rows that have other values\n",
    "    * we noticed mostly M get attacked\n",
    "    * percentages Male to Female victims\n",
    "    * We remove NULL DATE data rows that dont have a date or that the date doesnt include a month and a year\n",
    "    * We are gonna check the column of DATE and seperate it into three columns DAY + MONTH + YEAR\n",
    "    * We verify that the new YEAR column matches with the old YEAR column and keep the ones that match\n",
    "        * NEW YEAR COLUMN is the one split from the DATE column\n",
    "        * OLD YEAR COLUMN is the one already existing in the original sheet\n",
    "    * Once we finish comparing the new YEAR column vs old YEAR column and we find them not matching on some data rows. we remove the none matching ones so we keep clean data of accurate years\n",
    "    * We remove the DAY and OLD YEAR columns\n",
    "    * We are gonna keep the MONTH and matching YEAR\n",
    "\n",
    "**- AGE (victims age ranges)** \n",
    "\n",
    "    * majority of victims survive\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and 40+) \n",
    "    * keep in mind complications depending on age when getting treated\n",
    "    * percentages of victims based on age ranges\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and elders 40+) \n",
    "    * note that there are complications depending on age\n",
    "\n",
    "**- FATALITY ( Y or N )** @Cecilia\n",
    "\n",
    "    * mostly survived for the pharamaceutical logistics & transportation of injured people to the hospital\n",
    "    * assumption we have a percentage of survivals highest and we use it to sell for the\n",
    "    * We check unique values and make sure it is only two values Y or N and deleted all rows that have other values\n",
    "    * assumption we have a high percentage of survivals and we use it to sell the idea to profit from selling products to hospitals\n",
    "\n",
    "**- INJURY TYPE**\n",
    "\n",
    "    * clean the type of injury by severity\n",
    "    * seperate the injury type into different severity\n",
    "    * seperate the injury type into different body parts\n",
    "    * treatment depends on type of injury and thus the supplies as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f092e",
   "metadata": {},
   "source": [
    "## CLEAN THE DATA\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "961c8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_shark = pd.read_csv(\"/Users/blanca/Library/Mobile Documents/com~apple~CloudDocs/IRONHACK/github/02_project-shark-attacks/data/raw.csv\")\n",
    "\n",
    "data_shark.columns =[col.upper() for col in data_shark.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18d81baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australia' 'Bahamas' 'Seychelles' 'Argentina' 'Costa Rica' 'Brazil'\n",
      " 'Egypt' 'French Polynesia' 'USA' 'Colombia' 'South Africa' 'Spain'\n",
      " 'Comoros' 'Mexico' 'Cape Verde' 'Cayman Islands' 'Italy' 'Jamaica'\n",
      " 'Trinidad & Tobago' 'Canada' 'Croatia' 'Saudi Arabia' 'Antigua'\n",
      " 'United Arab Emirates (Uae)' 'Guam' 'Nevis' 'Japan' 'New Zealand'\n",
      " 'British Virgin Islands' 'Senegal' 'Belize' 'Liberia' 'Honduras'\n",
      " 'Sri Lanka' 'Indonesia' 'New Caledonia' 'Madagascar' 'Malaysia' 'Tonga'\n",
      " 'Bermuda' 'Montenegro' 'Somalia' 'Greece' 'Mozambique' 'Papua New Guinea'\n",
      " 'Tanzania' 'Panama' 'Philippines' 'Atlantic Ocean' 'Johnston Island'\n",
      " 'Marshall Islands' 'Caribbean Sea' 'Turkey' 'Cuba' 'Guatemala'\n",
      " 'North Atlantic Ocean' 'North Pacific Ocean' 'Pacific Ocean'\n",
      " 'Indian Ocean' 'UK' 'Israel' 'India' 'Haiti' 'Yemen' 'Crete' 'France'\n",
      " 'Syria' 'Azores' 'Fiji' 'Guyana' 'China' 'Norway' 'Iceland' 'Roatan'\n",
      " 'Guinea']\n"
     ]
    }
   ],
   "source": [
    "# COUNTRY\n",
    "\n",
    "data_shark = data_shark.dropna(subset=[\"COUNTRY\"])\n",
    "data_shark[\"COUNTRY\"] = data_shark[\"COUNTRY\"].str.strip().str.title()\n",
    "\n",
    "country_replacements = {\n",
    "    \"United States Of America\": \"USA\",\n",
    "    \"Usa\": \"USA\",\n",
    "    \"Us\": \"USA\",\n",
    "    \"U.s.\": \"USA\",\n",
    "    \"United Kingdom\": \"UK\",\n",
    "    \"England\": \"UK\",\n",
    "    \"Uk\":\"UK\",\n",
    "    \"Brasil\": \"Brazil\",\n",
    "    \"M√©xico\": \"Mexico\",\n",
    "}\n",
    "\n",
    "data_shark[\"COUNTRY\"] = data_shark[\"COUNTRY\"].replace(country_replacements)\n",
    "data_shark = data_shark[data_shark[\"COUNTRY\"] != \"Italy / Croatia\"]\n",
    "\n",
    "print(data_shark[\"COUNTRY\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "001c3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           DATE  DAY MONTH  YEAR\n",
      "0   25 Aug 2023  NaN   NaN   NaN\n",
      "1   21 Aug-2023  NaN   NaN   NaN\n",
      "2   07-Jun-2023  NaN   NaN   NaN\n",
      "3   02-Mar-2023  NaN   NaN   NaN\n",
      "4   18-Feb-2023  NaN   NaN   NaN\n",
      "5   08-Feb-2022  NaN   NaN   NaN\n",
      "6   15-Nov-2021  NaN   NaN   NaN\n",
      "7   16-Oct-2021  NaN   NaN   NaN\n",
      "8   10-Sep-2021  NaN   NaN   NaN\n",
      "9   29-Jul-2020  NaN   NaN   NaN\n",
      "10  06-Jun-2020  NaN   NaN   NaN\n",
      "11  20-Dec-2019  NaN   NaN   NaN\n",
      "12  21-Oct-2019  NaN   NaN   NaN\n",
      "13  28-Jul-2019  NaN   NaN   NaN\n",
      "14  18-Jul-2019  NaN   NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "# DATE \n",
    "\n",
    "#TO HAVE THE COMPLEATE DATE.\n",
    "data_shark[\"Full_Date\"] = data_shark[\"DATE\"].astype(str) + \" \" + data_shark[\"YEAR\"].astype(str)\n",
    "\n",
    "#WE ELIMINATE SUFIX AS 1ST , 2ND ....\n",
    "data_shark[\"Full_Date\"] = (\n",
    "    data_shark[\"Full_Date\"]\n",
    "    .str.replace(\"[-_/]\", \" \", regex=False)\n",
    "    .str.replace(\"st\", \"\", regex=False)\n",
    "    .str.replace(\"nd\", \"\", regex=False)\n",
    "    .str.replace(\"rd\", \"\", regex=False)\n",
    "    .str.replace(\"th\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")   \n",
    "\n",
    "#WE ASK PANDA TO TRY TO READ THE DATE \n",
    "data_shark[\"Full_Date\"] = pd.to_datetime(\n",
    "    data_shark[\"Full_Date\"], \n",
    "    format=\"%d %b %Y\",   \n",
    "    errors=\"coerce\", \n",
    "    dayfirst=True\n",
    ")\n",
    "\n",
    "#WE CREATE OUR COLUMNS\n",
    "data_shark[\"DAY\"] = data_shark[\"Full_Date\"].dt.day\n",
    "data_shark[\"MONTH\"] = data_shark[\"Full_Date\"].dt.month_name()  # Example: October, February\n",
    "data_shark[\"YEAR\"] = data_shark[\"Full_Date\"].dt.year\n",
    "\n",
    "print(data_shark[[\"DATE\", \"DAY\", \"MONTH\", \"YEAR\"]].head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
