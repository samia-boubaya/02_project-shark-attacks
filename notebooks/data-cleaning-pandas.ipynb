{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c1bffa",
   "metadata": {},
   "source": [
    "# PROJECT 02 : SHARK ATTACKS\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a804f",
   "metadata": {},
   "source": [
    "### BRIEF : data analysis of global shark attacks for a business idea\n",
    "You will initially examine the Shark Attack dataset, understanding its structure and formulating a hypothesis or several hypotheses about the data. \n",
    " We hypothesize that shark attacks are more common in certain locations and peak during specific months.\n",
    "We define a Business Case, such as \n",
    "\n",
    "* ‚ÄúAs a company that sells medical products, I want to identify destinations with high shark attack rates.‚Äù\n",
    "* ‚ÄúAs a company providing supply transportation services, I want to know when and where shark attacks peak to plan the safe transport of medical supplies to hospitals.‚Äù\n",
    "\n",
    " Throughout the project, we will use Python and the pandas library to apply at least five data cleaning techniques to handle missing values, duplicates, and formatting inconsistencies. After cleaning, we will perform basic exploratory data analysis to validate our hypotheses and extract insights. \n",
    "\n",
    "#### üìù BUSINESS IDEA ‚Äî 3 Bullet Points\n",
    "\n",
    "* Problem to Solve: Coastal hospitals and emergency response teams are not always prepared with the right medical supplies during periods of high shark-attack frequency. This business solves the problem by predicting when and where attacks are most likely, so medical supplies can be stocked in advance.\n",
    "\n",
    "* Business Concept: Use historical shark attack data to create global heatmaps and seasonal risk forecasts. Then provide pharmaceutical products (painkillers, antibiotics, blood bags, emergency kits) and transportation support to hospitals and ambulances near high-risk beaches.\n",
    "\n",
    "* Data Used to Profit: The business will analyze Country, Date/Month, Gender, Age, Fatality, and Type of Injury to identify high-risk locations, peak attack months, and most common injury types. This allows optimized supply production, targeted sales, and efficient delivery to the areas that need it most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736f644",
   "metadata": {},
   "source": [
    "## üåÄCOLUMNS TO CLEAN : \n",
    "-----\n",
    "\n",
    "**- COUNTRY (global comparison between countries to invest in more)** @Blanca\n",
    "\n",
    "    * we just take the country column and make sure every country name is accurately named\n",
    "    * We are gonna check the column of COUNTRY and make sure every country name is correct\n",
    "    * We remove NULL COUNTRY data rows that dont have any country\n",
    "    * We need to make sure that the name of COUNTRY  is capitalized and written the same way for example :\n",
    "        United States of Amercia == USA == US\n",
    "        it has to have the same name and consistent!\n",
    "\n",
    "**- DATE ( MONTH + YEAR )** @Blanca\n",
    "\n",
    "    * split the date into day - month columns and only use month column\n",
    "    * interpret months which have shark attacks happen the most\n",
    "\n",
    "**- GENDER ( F or M )** @Cecilia\n",
    "\n",
    "    * We check unique values and make it so it is only two values F or M and deleted all rows that have other values\n",
    "    * we noticed mostly M get attacked\n",
    "    * percentages Male to Female victims\n",
    "    * We remove NULL DATE data rows that dont have a date or that the date doesnt include a month and a year\n",
    "    * We are gonna check the column of DATE and seperate it into three columns DAY + MONTH + YEAR\n",
    "    * We verify that the new YEAR column matches with the old YEAR column and keep the ones that match\n",
    "        * NEW YEAR COLUMN is the one split from the DATE column\n",
    "        * OLD YEAR COLUMN is the one already existing in the original sheet\n",
    "    * Once we finish comparing the new YEAR column vs old YEAR column and we find them not matching on some data rows. we remove the none matching ones so we keep clean data of accurate years\n",
    "    * We remove the DAY and OLD YEAR columns\n",
    "    * We are gonna keep the MONTH and matching YEAR\n",
    "\n",
    "**- AGE (victims age ranges)** @Samia\n",
    "\n",
    "    * majority of victims survive\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and 40+) \n",
    "    * keep in mind complications depending on age when getting treated\n",
    "    * percentages of victims based on age ranges\n",
    "    * we split the age groups into three categories (minors under 18 / adults 18-40 and elders 40+) \n",
    "    * note that there are complications depending on age\n",
    "\n",
    "**- FATALITY ( Y or N )** @Cecilia\n",
    "    * depends on the column 'Type' of injury and if it includes death or high severity\n",
    "    * mostly survived \n",
    "    * for the pharamaceutical logistics & transportation of injured people to the hospital\n",
    "    * assumption we have a percentage of survivals highest and we use it to sell for the\n",
    "    * We check unique values and make sure it is only two values Y or N and deleted all rows that have other values\n",
    "    * assumption we have a high percentage of survivals and we use it to sell the idea to profit from selling products to hospitals\n",
    "\n",
    "**- INJURY TYPE** @Samia\n",
    "\n",
    "    * clean the type of injury by severity\n",
    "    * seperate the injury type into different severity\n",
    "    * seperate the injury type into different body parts\n",
    "    * treatment depends on type of injury and thus the supplies as well\n",
    "\n",
    "-------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9f7f7",
   "metadata": {},
   "source": [
    "### IMPORT UTILS and INIT from SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "940f52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src import utils\n",
    "from src import init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff922e82",
   "metadata": {},
   "source": [
    "-----\n",
    "### GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "53e2d19d",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:1319\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1319\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1320\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1338\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1384\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1383\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1333\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1093\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1093\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1037\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\http\\client.py:1479\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1477\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock,\n\u001b[0;32m   1480\u001b[0m                                       server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\ssl.py:1076\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1075\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1076\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\ssl.py:1372\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[680], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?export=download&id=1ZRuzHohahE54BC7hKcA289tyuL9cp7-h\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:189\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, context)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:489\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    486\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    488\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 489\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    492\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:506\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    505\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 506\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    507\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:466\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    465\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 466\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:1367\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1368\u001b[0m                         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n",
      "File \u001b[1;32mc:\\Users\\sboub\\anaconda3\\Lib\\urllib\\request.py:1322\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1320\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1323\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "url = 'https://drive.google.com/uc?export=download&id=1ZRuzHohahE54BC7hKcA289tyuL9cp7-h'\n",
    "df = pd.read_csv(url)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option 1: Display the entire DataFrame (all rows and columns)\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "\n",
    "#the full table\n",
    "#print(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c051de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 561 entries, 0 to 560\n",
      "Data columns (total 15 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       561 non-null    object \n",
      " 1   Year       560 non-null    float64\n",
      " 2   Type       557 non-null    object \n",
      " 3   Country    556 non-null    object \n",
      " 4   State      508 non-null    object \n",
      " 5   Location   503 non-null    object \n",
      " 6   Activity   433 non-null    object \n",
      " 7   Name       502 non-null    object \n",
      " 8   Sex        480 non-null    object \n",
      " 9   Age        238 non-null    object \n",
      " 10  Injury     553 non-null    object \n",
      " 11  Fatal Y/N  0 non-null      float64\n",
      " 12  Time       157 non-null    object \n",
      " 13  Species    526 non-null    object \n",
      " 14  Source     556 non-null    object \n",
      "dtypes: float64(2), object(13)\n",
      "memory usage: 65.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# change type of each serie\n",
    "# year into int or split it from Date\n",
    "# Age into int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Frame SHAPE (561, 15)\n",
      "\n",
      "Data Columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity',\n",
       "       'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species ',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nData Frame SHAPE\", df.shape)\n",
    "\n",
    "print(\"\\nData Columns\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837e2e3",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Country' CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da6a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity',\n",
       "       'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species ',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795723f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       AUSTRALIA\n",
       "1                         BAHAMAS\n",
       "2                         BAHAMAS\n",
       "3                      SEYCHELLES\n",
       "4                       ARGENTINA\n",
       "5                      COSTA RICA\n",
       "6                          BRAZIL\n",
       "7                       AUSTRALIA\n",
       "8                           EGYPT\n",
       "9                       AUSTRALIA\n",
       "10                      AUSTRALIA\n",
       "11                      AUSTRALIA\n",
       "12               FRENCH POLYNESIA\n",
       "13                      AUSTRALIA\n",
       "14                            USA\n",
       "15                        BAHAMAS\n",
       "16                            USA\n",
       "17                       COLOMBIA\n",
       "18                         BRAZIL\n",
       "19                   SOUTH AFRICA\n",
       "20                            USA\n",
       "21                   SOUTH AFRICA\n",
       "22                          SPAIN\n",
       "23                            USA\n",
       "24                            USA\n",
       "25                        COMOROS\n",
       "26                            USA\n",
       "27                            USA\n",
       "28                            USA\n",
       "29                            USA\n",
       "30                   SOUTH AFRICA\n",
       "31                      AUSTRALIA\n",
       "32                      AUSTRALIA\n",
       "33                            USA\n",
       "34                         MEXICO\n",
       "35                         MEXICO\n",
       "36                            USA\n",
       "37                      AUSTRALIA\n",
       "38                     CAPE VERDE\n",
       "39                            USA\n",
       "40                            USA\n",
       "41                            USA\n",
       "42                 CAYMAN ISLANDS\n",
       "43                      AUSTRALIA\n",
       "44                            USA\n",
       "45                            USA\n",
       "46                            USA\n",
       "47                          SPAIN\n",
       "48                      AUSTRALIA\n",
       "49                            USA\n",
       "50                            USA\n",
       "51               FRENCH POLYNESIA\n",
       "52                            USA\n",
       "53                          ITALY\n",
       "54                         BRAZIL\n",
       "55                      AUSTRALIA\n",
       "56                            USA\n",
       "57                   SOUTH AFRICA\n",
       "58                            USA\n",
       "59                            USA\n",
       "60                            NaN\n",
       "61                        BAHAMAS\n",
       "62                            USA\n",
       "63                      AUSTRALIA\n",
       "64                      AUSTRALIA\n",
       "65                 CAYMAN ISLANDS\n",
       "66                            USA\n",
       "67                          SPAIN\n",
       "68                        JAMAICA\n",
       "69                            USA\n",
       "70                      AUSTRALIA\n",
       "71                      AUSTRALIA\n",
       "72                      AUSTRALIA\n",
       "73                      AUSTRALIA\n",
       "74                            USA\n",
       "75                            USA\n",
       "76              TRINIDAD & TOBAGO\n",
       "77                         CANADA\n",
       "78                      AUSTRALIA\n",
       "79                        CROATIA\n",
       "80                            USA\n",
       "81                   SAUDI ARABIA\n",
       "82                        ANTIGUA\n",
       "83                   SOUTH AFRICA\n",
       "84                      AUSTRALIA\n",
       "85                            USA\n",
       "86                      AUSTRALIA\n",
       "87                            USA\n",
       "88                         BRAZIL\n",
       "89     UNITED ARAB EMIRATES (UAE)\n",
       "90                          EGYPT\n",
       "91                      AUSTRALIA\n",
       "92                      AUSTRALIA\n",
       "93                        BAHAMAS\n",
       "94                            USA\n",
       "95                           GUAM\n",
       "96                   SOUTH AFRICA\n",
       "97                            USA\n",
       "98                          NEVIS\n",
       "99                          SPAIN\n",
       "100                           USA\n",
       "101                           USA\n",
       "102                           USA\n",
       "103                  SOUTH AFRICA\n",
       "104                        BRAZIL\n",
       "105                        BRAZIL\n",
       "106                     AUSTRALIA\n",
       "107                           USA\n",
       "108                           USA\n",
       "109                           USA\n",
       "110                           USA\n",
       "111                           USA\n",
       "112                  SOUTH AFRICA\n",
       "113                        MEXICO\n",
       "114                     AUSTRALIA\n",
       "115                         JAPAN\n",
       "116                   NEW ZEALAND\n",
       "117        BRITISH VIRGIN ISLANDS\n",
       "118                     AUSTRALIA\n",
       "119                        MEXICO\n",
       "120                           USA\n",
       "121                           USA\n",
       "122                       SENEGAL\n",
       "123                           USA\n",
       "124                     AUSTRALIA\n",
       "125                    COSTA RICA\n",
       "126                     AUSTRALIA\n",
       "127                           USA\n",
       "128              FRENCH POLYNESIA\n",
       "129                  SOUTH AFRICA\n",
       "130                     AUSTRALIA\n",
       "131                        BRAZIL\n",
       "132                        BELIZE\n",
       "133                     AUSTRALIA\n",
       "134                         SPAIN\n",
       "135                           USA\n",
       "136                  SOUTH AFRICA\n",
       "137                           USA\n",
       "138                       BAHAMAS\n",
       "139                           USA\n",
       "140                           USA\n",
       "141                  SOUTH AFRICA\n",
       "142                    Seychelles\n",
       "143                           USA\n",
       "144                       LIBERIA\n",
       "145                     AUSTRALIA\n",
       "146                           USA\n",
       "147                           USA\n",
       "148                           USA\n",
       "149                      HONDURAS\n",
       "150                  SOUTH AFRICA\n",
       "151                     SRI LANKA\n",
       "152                   NEW ZEALAND\n",
       "153                           USA\n",
       "154                     AUSTRALIA\n",
       "155                           USA\n",
       "156                           USA\n",
       "157                     AUSTRALIA\n",
       "158                   NEW ZEALAND\n",
       "159                   NEW ZEALAND\n",
       "160                           USA\n",
       "161                           USA\n",
       "162                           USA\n",
       "163                           USA\n",
       "164                           USA\n",
       "165                  SOUTH AFRICA\n",
       "166                     AUSTRALIA\n",
       "167                     INDONESIA\n",
       "168                     AUSTRALIA\n",
       "169                 NEW CALEDONIA\n",
       "170                     AUSTRALIA\n",
       "171                     AUSTRALIA\n",
       "172                    MADAGASCAR\n",
       "173                        BRAZIL\n",
       "174                      MALAYSIA\n",
       "175                     AUSTRALIA\n",
       "176                           USA\n",
       "177                           USA\n",
       "178                           USA\n",
       "179                     AUSTRALIA\n",
       "180                   NEW ZEALAND\n",
       "181                           USA\n",
       "182                     AUSTRALIA\n",
       "183                           USA\n",
       "184                         ITALY\n",
       "185                     AUSTRALIA\n",
       "186                           USA\n",
       "187                     AUSTRALIA\n",
       "188                        MEXICO\n",
       "189                           USA\n",
       "190                           USA\n",
       "191                           USA\n",
       "192                           USA\n",
       "193                         TONGA\n",
       "194                           USA\n",
       "195                           USA\n",
       "196                  SOUTH AFRICA\n",
       "197                           USA\n",
       "198                     AUSTRALIA\n",
       "199                           USA\n",
       "200                     AUSTRALIA\n",
       "201                           USA\n",
       "202                           USA\n",
       "203                           USA\n",
       "204                           USA\n",
       "205                     AUSTRALIA\n",
       "206                         JAPAN\n",
       "207                       BAHAMAS\n",
       "208                  SOUTH AFRICA\n",
       "209                           USA\n",
       "210                     AUSTRALIA\n",
       "211                           USA\n",
       "212                           USA\n",
       "213                           USA\n",
       "214                       BERMUDA\n",
       "215                         JAPAN\n",
       "216                           USA\n",
       "217                           USA\n",
       "218                  SOUTH AFRICA\n",
       "219                  SOUTH AFRICA\n",
       "220                     AUSTRALIA\n",
       "221                     AUSTRALIA\n",
       "222                  SOUTH AFRICA\n",
       "223                  SOUTH AFRICA\n",
       "224                  SOUTH AFRICA\n",
       "225                    MONTENEGRO\n",
       "226                  SOUTH AFRICA\n",
       "227                           USA\n",
       "228                  SOUTH AFRICA\n",
       "229                  SOUTH AFRICA\n",
       "230                           USA\n",
       "231                           USA\n",
       "232                           USA\n",
       "233                  SOUTH AFRICA\n",
       "234                           USA\n",
       "235                           USA\n",
       "236                           USA\n",
       "237                  SOUTH AFRICA\n",
       "238                       SOMALIA\n",
       "239                  SOUTH AFRICA\n",
       "240                        GREECE\n",
       "241                  SOUTH AFRICA\n",
       "242                  SOUTH AFRICA\n",
       "243                         ITALY\n",
       "244                  SOUTH AFRICA\n",
       "245                  SOUTH AFRICA\n",
       "246                           USA\n",
       "247                           USA\n",
       "248                  SOUTH AFRICA\n",
       "249                           USA\n",
       "250                           USA\n",
       "251                           USA\n",
       "252                           USA\n",
       "253                  SOUTH AFRICA\n",
       "254                  SOUTH AFRICA\n",
       "255                  SOUTH AFRICA\n",
       "256                        GREECE\n",
       "257                  SOUTH AFRICA\n",
       "258                           USA\n",
       "259                  SOUTH AFRICA\n",
       "260                         ITALY\n",
       "261                  SOUTH AFRICA\n",
       "262                           USA\n",
       "263                     AUSTRALIA\n",
       "264                     AUSTRALIA\n",
       "265                     AUSTRALIA\n",
       "266                  SOUTH AFRICA\n",
       "267                           USA\n",
       "268                           USA\n",
       "269                           USA\n",
       "270                         ITALY\n",
       "271                  SOUTH AFRICA\n",
       "272                  SOUTH AFRICA\n",
       "273                    MOZAMBIQUE\n",
       "274                           USA\n",
       "275                       CROATIA\n",
       "276                           NaN\n",
       "277                           USA\n",
       "278                     AUSTRALIA\n",
       "279                     AUSTRALIA\n",
       "280                  SOUTH AFRICA\n",
       "281                  SOUTH AFRICA\n",
       "282              PAPUA NEW GUINEA\n",
       "283                           USA\n",
       "284                        MEXICO\n",
       "285                      TANZANIA\n",
       "286                           USA\n",
       "287                           USA\n",
       "288                           USA\n",
       "289                           USA\n",
       "290              PAPUA NEW GUINEA\n",
       "291                  SOUTH AFRICA\n",
       "292                           USA\n",
       "293                         ITALY\n",
       "294                     AUSTRALIA\n",
       "295                           USA\n",
       "296                     AUSTRALIA\n",
       "297                     AUSTRALIA\n",
       "298                       BAHAMAS\n",
       "299                  SOUTH AFRICA\n",
       "300                           USA\n",
       "301                           USA\n",
       "302                           USA\n",
       "303                           USA\n",
       "304                        PANAMA\n",
       "305                   PHILIPPINES\n",
       "306                           USA\n",
       "307                           USA\n",
       "308                           USA\n",
       "309                           USA\n",
       "310                           USA\n",
       "311                  SOUTH AFRICA\n",
       "312                           USA\n",
       "313                           NaN\n",
       "314                           USA\n",
       "315                           USA\n",
       "316                           USA\n",
       "317                  SOUTH AFRICA\n",
       "318                     AUSTRALIA\n",
       "319                   NEW ZEALAND\n",
       "320                ATLANTIC OCEAN\n",
       "321                           USA\n",
       "322                           USA\n",
       "323                           USA\n",
       "324                           USA\n",
       "325                     AUSTRALIA\n",
       "326                     AUSTRALIA\n",
       "327                           USA\n",
       "328                        GREECE\n",
       "329                           USA\n",
       "330                  SOUTH AFRICA\n",
       "331                     AUSTRALIA\n",
       "332                        MEXICO\n",
       "333                           USA\n",
       "334                     AUSTRALIA\n",
       "335                           USA\n",
       "336                           USA\n",
       "337                  SOUTH AFRICA\n",
       "338                           USA\n",
       "339                     AUSTRALIA\n",
       "340                           USA\n",
       "341               JOHNSTON ISLAND\n",
       "342              MARSHALL ISLANDS\n",
       "343              MARSHALL ISLANDS\n",
       "344                           USA\n",
       "345                    MOZAMBIQUE\n",
       "346                           USA\n",
       "347                       BERMUDA\n",
       "348                           USA\n",
       "349                           USA\n",
       "350                 CARIBBEAN SEA\n",
       "351                     AUSTRALIA\n",
       "352                           USA\n",
       "353                       BERMUDA\n",
       "354                           USA\n",
       "355                     AUSTRALIA\n",
       "356                         JAPAN\n",
       "357                   PHILIPPINES\n",
       "358                           USA\n",
       "359                           USA\n",
       "360                         ITALY\n",
       "361                           USA\n",
       "362                     AUSTRALIA\n",
       "363                        TURKEY\n",
       "364                  SOUTH AFRICA\n",
       "365                     AUSTRALIA\n",
       "366                  SOUTH AFRICA\n",
       "367                     AUSTRALIA\n",
       "368                         JAPAN\n",
       "369                          CUBA\n",
       "370                     SRI LANKA\n",
       "371                          CUBA\n",
       "372                           USA\n",
       "373                         JAPAN\n",
       "374                         ITALY\n",
       "375                       CROATIA\n",
       "376                           USA\n",
       "377                        MEXICO\n",
       "378                     GUATEMALA\n",
       "379                  SOUTH AFRICA\n",
       "380                    MOZAMBIQUE\n",
       "381          NORTH ATLANTIC OCEAN\n",
       "382                     AUSTRALIA\n",
       "383                           USA\n",
       "384                     AUSTRALIA\n",
       "385                           USA\n",
       "386                           USA\n",
       "387                     AUSTRALIA\n",
       "388                         ITALY\n",
       "389                     AUSTRALIA\n",
       "390                     AUSTRALIA\n",
       "391                       SENEGAL\n",
       "392                  SOUTH AFRICA\n",
       "393                        GREECE\n",
       "394                     AUSTRALIA\n",
       "395           NORTH PACIFIC OCEAN\n",
       "396                           USA\n",
       "397                        MEXICO\n",
       "398                 PACIFIC OCEAN\n",
       "399                           USA\n",
       "400                        BRAZIL\n",
       "401                  INDIAN OCEAN\n",
       "402                     AUSTRALIA\n",
       "403                     AUSTRALIA\n",
       "404                  SOUTH AFRICA\n",
       "405                  SOUTH AFRICA\n",
       "406                  SOUTH AFRICA\n",
       "407                     AUSTRALIA\n",
       "408                     AUSTRALIA\n",
       "409                        TURKEY\n",
       "410                         ITALY\n",
       "411                     AUSTRALIA\n",
       "412                     AUSTRALIA\n",
       "413                     AUSTRALIA\n",
       "414                     AUSTRALIA\n",
       "415                UNITED KINGDOM\n",
       "416                           USA\n",
       "417                           USA\n",
       "418                     AUSTRALIA\n",
       "419                        ISRAEL\n",
       "420                     AUSTRALIA\n",
       "421               ITALY / CROATIA\n",
       "422                     AUSTRALIA\n",
       "423                           USA\n",
       "424                           USA\n",
       "425                           USA\n",
       "426                     AUSTRALIA\n",
       "427                    COSTA RICA\n",
       "428                  SOUTH AFRICA\n",
       "429                    CAPE VERDE\n",
       "430                     AUSTRALIA\n",
       "431                           USA\n",
       "432                         INDIA\n",
       "433                           USA\n",
       "434                           USA\n",
       "435                         SPAIN\n",
       "436                     AUSTRALIA\n",
       "437                     AUSTRALIA\n",
       "438                     AUSTRALIA\n",
       "439                     AUSTRALIA\n",
       "440                     AUSTRALIA\n",
       "441                     AUSTRALIA\n",
       "442                          CUBA\n",
       "443                     AUSTRALIA\n",
       "444                     AUSTRALIA\n",
       "445                   PHILIPPINES\n",
       "446                           USA\n",
       "447                           USA\n",
       "448                         EGYPT\n",
       "449                   NEW ZEALAND\n",
       "450                           USA\n",
       "451                           USA\n",
       "452                           USA\n",
       "453                           USA\n",
       "454                         ITALY\n",
       "455                  SOUTH AFRICA\n",
       "456                           USA\n",
       "457                       CROATIA\n",
       "458                           USA\n",
       "459                           USA\n",
       "460                           USA\n",
       "461                           USA\n",
       "462                           NaN\n",
       "463                         ITALY\n",
       "464                         SPAIN\n",
       "465                           USA\n",
       "466                         HAITI\n",
       "467                       ENGLAND\n",
       "468                     AUSTRALIA\n",
       "469                    MOZAMBIQUE\n",
       "470                     AUSTRALIA\n",
       "471                   NEW ZEALAND\n",
       "472                     AUSTRALIA\n",
       "473                         EGYPT\n",
       "474                         ITALY\n",
       "475                     AUSTRALIA\n",
       "476                     AUSTRALIA\n",
       "477                     AUSTRALIA\n",
       "478                           USA\n",
       "479                  SOUTH AFRICA\n",
       "480                  SOUTH AFRICA\n",
       "481                         ITALY\n",
       "482                        YEMEN \n",
       "483                  SOUTH AFRICA\n",
       "484                           USA\n",
       "485                   NEW ZEALAND\n",
       "486                   PHILIPPINES\n",
       "487                     AUSTRALIA\n",
       "488                           USA\n",
       "489                     AUSTRALIA\n",
       "490                         CRETE\n",
       "491                        MEXICO\n",
       "492                          CUBA\n",
       "493                       CROATIA\n",
       "494                     AUSTRALIA\n",
       "495                        FRANCE\n",
       "496                           USA\n",
       "497                     AUSTRALIA\n",
       "498                  SOUTH AFRICA\n",
       "499                        GREECE\n",
       "500                           USA\n",
       "501                       JAMAICA\n",
       "502                       CROATIA\n",
       "503                     AUSTRALIA\n",
       "504                       CROATIA\n",
       "505                           USA\n",
       "506                   NEW ZEALAND\n",
       "507                   NEW ZEALAND\n",
       "508                     AUSTRALIA\n",
       "509                           USA\n",
       "510                           USA\n",
       "511                     AUSTRALIA\n",
       "512                           USA\n",
       "513                        MEXICO\n",
       "514                         SYRIA\n",
       "515                           USA\n",
       "516                     AUSTRALIA\n",
       "517                  INDIAN OCEAN\n",
       "518                  SOUTH AFRICA\n",
       "519                  SOUTH AFRICA\n",
       "520                     AUSTRALIA\n",
       "521                     AUSTRALIA\n",
       "522                        AZORES\n",
       "523                       CROATIA\n",
       "524                          Fiji\n",
       "525                     AUSTRALIA\n",
       "526                        GUYANA\n",
       "527                     AUSTRALIA\n",
       "528                           NaN\n",
       "529                     AUSTRALIA\n",
       "530                         CHINA\n",
       "531                     AUSTRALIA\n",
       "532                  SOUTH AFRICA\n",
       "533                   NEW ZEALAND\n",
       "534                         SPAIN\n",
       "535                           USA\n",
       "536                           USA\n",
       "537                     AUSTRALIA\n",
       "538                           USA\n",
       "539                     AUSTRALIA\n",
       "540                ATLANTIC OCEAN\n",
       "541                     AUSTRALIA\n",
       "542                           USA\n",
       "543                           USA\n",
       "544                     AUSTRALIA\n",
       "545                           USA\n",
       "546                         SPAIN\n",
       "547                     AUSTRALIA\n",
       "548                       ENGLAND\n",
       "549                       JAMAICA\n",
       "550                         SPAIN\n",
       "551                        NORWAY\n",
       "552                       ANTIGUA\n",
       "553                           USA\n",
       "554                           USA\n",
       "555                        FRANCE\n",
       "556                       ICELAND\n",
       "557                        ROATAN\n",
       "558                        GUINEA\n",
       "559                     AUSTRALIA\n",
       "560                           USA\n",
       "Name: Country, dtype: object"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba694fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AUSTRALIA', 'BAHAMAS', 'SEYCHELLES', 'ARGENTINA', 'COSTA RICA',\n",
       "       'BRAZIL', 'EGYPT', 'FRENCH POLYNESIA', 'USA', 'COLOMBIA',\n",
       "       'SOUTH AFRICA', 'SPAIN', 'COMOROS', 'MEXICO', 'CAPE VERDE',\n",
       "       'CAYMAN ISLANDS', 'ITALY', nan, 'JAMAICA', 'TRINIDAD & TOBAGO',\n",
       "       'CANADA', 'CROATIA', 'SAUDI ARABIA', 'ANTIGUA',\n",
       "       'UNITED ARAB EMIRATES (UAE)', 'GUAM', 'NEVIS', 'JAPAN',\n",
       "       'NEW ZEALAND', 'BRITISH VIRGIN ISLANDS', 'SENEGAL', 'BELIZE',\n",
       "       'Seychelles', 'LIBERIA', 'HONDURAS', 'SRI LANKA', 'INDONESIA',\n",
       "       'NEW CALEDONIA', 'MADAGASCAR', 'MALAYSIA', 'TONGA', 'BERMUDA',\n",
       "       'MONTENEGRO', 'SOMALIA', 'GREECE', 'MOZAMBIQUE',\n",
       "       'PAPUA NEW GUINEA', 'TANZANIA', 'PANAMA', 'PHILIPPINES',\n",
       "       'ATLANTIC OCEAN', 'JOHNSTON ISLAND', 'MARSHALL ISLANDS',\n",
       "       'CARIBBEAN SEA', 'TURKEY', 'CUBA', 'GUATEMALA',\n",
       "       'NORTH ATLANTIC OCEAN', 'NORTH PACIFIC OCEAN', 'PACIFIC OCEAN',\n",
       "       'INDIAN OCEAN', 'UNITED KINGDOM', 'ISRAEL', 'ITALY / CROATIA',\n",
       "       'INDIA', 'HAITI', 'ENGLAND', 'YEMEN ', 'CRETE', 'FRANCE', 'SYRIA',\n",
       "       'AZORES', 'Fiji', 'GUYANA', 'CHINA', 'NORWAY', 'ICELAND', 'ROATAN',\n",
       "       'GUINEA'], dtype=object)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country'].nunique()\n",
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     555\n",
       "unique     75\n",
       "top       USA\n",
       "freq      191\n",
       "Name: Country, dtype: object"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COUNTRY\n",
    "\n",
    "df = df.dropna(subset=[\"Country\"])\n",
    "df[\"Country\"] = df[\"Country\"].str.strip().str.title()\n",
    "\n",
    "country_replacements = {\n",
    "    \"United States Of America\": \"USA\",\n",
    "    \"Usa\": \"USA\",\n",
    "    \"Us\": \"USA\",\n",
    "    \"U.s.\": \"USA\",\n",
    "    \"United Kingdom\": \"UK\",\n",
    "    \"England\": \"UK\",\n",
    "    \"Uk\":\"UK\",\n",
    "    \"Brasil\": \"Brazil\",\n",
    "    \"M√©xico\": \"Mexico\",\n",
    "    \"United Arab Emirates (Uae)\": \"UAE\",\n",
    "    \"Turkey\":\"T√ºrkiye\"\n",
    "}\n",
    "\n",
    "df[\"Country\"] = df[\"Country\"].replace(country_replacements)\n",
    "df = df[df[\"Country\"] != \"Italy / Croatia\"]\n",
    "\n",
    "df[\"Country\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95045f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Australia', 'Bahamas', 'Seychelles', 'Argentina', 'Costa Rica',\n",
       "       'Brazil', 'Egypt', 'French Polynesia', 'USA', 'Colombia',\n",
       "       'South Africa', 'Spain', 'Comoros', 'Mexico', 'Cape Verde',\n",
       "       'Cayman Islands', 'Italy', 'Jamaica', 'Trinidad & Tobago',\n",
       "       'Canada', 'Croatia', 'Saudi Arabia', 'Antigua', 'UAE', 'Guam',\n",
       "       'Nevis', 'Japan', 'New Zealand', 'British Virgin Islands',\n",
       "       'Senegal', 'Belize', 'Liberia', 'Honduras', 'Sri Lanka',\n",
       "       'Indonesia', 'New Caledonia', 'Madagascar', 'Malaysia', 'Tonga',\n",
       "       'Bermuda', 'Montenegro', 'Somalia', 'Greece', 'Mozambique',\n",
       "       'Papua New Guinea', 'Tanzania', 'Panama', 'Philippines',\n",
       "       'Atlantic Ocean', 'Johnston Island', 'Marshall Islands',\n",
       "       'Caribbean Sea', 'T√ºrkiye', 'Cuba', 'Guatemala',\n",
       "       'North Atlantic Ocean', 'North Pacific Ocean', 'Pacific Ocean',\n",
       "       'Indian Ocean', 'UK', 'Israel', 'India', 'Haiti', 'Yemen', 'Crete',\n",
       "       'France', 'Syria', 'Azores', 'Fiji', 'Guyana', 'China', 'Norway',\n",
       "       'Iceland', 'Roatan', 'Guinea'], dtype=object)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "USA             191\n",
      "Australia       120\n",
      "South Africa     64\n",
      "Italy            14\n",
      "New Zealand      12\n",
      "Mexico           11\n",
      "Spain            10\n",
      "Brazil            9\n",
      "Bahamas           8\n",
      "Croatia           8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_countries = df['Country'].value_counts().head(10)\n",
    "print(top_countries)\n",
    "# PIE CHART FOR COUNTRIES WITH HIGHEST SHARK ATTACKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save COUNTRY CLEANED file\n",
    "df.to_csv(\"../data/interim/1_interim_country.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301f9be",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Sex' CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a368dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity',\n",
       "       'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species ',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        M\n",
       "1        M\n",
       "2        F\n",
       "3        M\n",
       "4        M\n",
       "5        F\n",
       "6        M\n",
       "7        M\n",
       "8        M\n",
       "9      NaN\n",
       "10       F\n",
       "11     NaN\n",
       "12       F\n",
       "13       F\n",
       "14     NaN\n",
       "15       M\n",
       "16       F\n",
       "17       M\n",
       "18       M\n",
       "19       M\n",
       "20       M\n",
       "21       M\n",
       "22       F\n",
       "23       M\n",
       "24       F\n",
       "25     NaN\n",
       "26       M\n",
       "27       F\n",
       "28       F\n",
       "29       M\n",
       "30       M\n",
       "31       M\n",
       "32       M\n",
       "33       M\n",
       "34       M\n",
       "35     NaN\n",
       "36       M\n",
       "37       M\n",
       "38       M\n",
       "39       M\n",
       "40       M\n",
       "41     NaN\n",
       "42       M\n",
       "43       M\n",
       "44       F\n",
       "45       M\n",
       "46       M\n",
       "47       M\n",
       "48       M\n",
       "49       F\n",
       "50       M\n",
       "51       M\n",
       "52       F\n",
       "53       M\n",
       "54       M\n",
       "55       F\n",
       "56       M\n",
       "57     NaN\n",
       "58       M\n",
       "59     NaN\n",
       "61       M\n",
       "62       M\n",
       "63       M\n",
       "64       F\n",
       "65       M\n",
       "66       F\n",
       "67       M\n",
       "68       M\n",
       "69       M\n",
       "70       M\n",
       "71       M\n",
       "72       M\n",
       "73       M\n",
       "74       M\n",
       "75       M\n",
       "76       M\n",
       "77       F\n",
       "78       F\n",
       "79     NaN\n",
       "80       M\n",
       "81       M\n",
       "82       M\n",
       "83       M\n",
       "84     NaN\n",
       "85       M\n",
       "86       M\n",
       "87       M\n",
       "88       M\n",
       "89       M\n",
       "90       F\n",
       "91       F\n",
       "92       M\n",
       "93       M\n",
       "94       F\n",
       "95       M\n",
       "96       M\n",
       "97       M\n",
       "98       M\n",
       "99       F\n",
       "100      M\n",
       "101      M\n",
       "102      M\n",
       "103      M\n",
       "104      M\n",
       "105      M\n",
       "106    NaN\n",
       "107      M\n",
       "108      F\n",
       "109      M\n",
       "110      M\n",
       "111      M\n",
       "112      M\n",
       "113      M\n",
       "114      M\n",
       "115      M\n",
       "116      M\n",
       "117      M\n",
       "118      M\n",
       "119      M\n",
       "120      F\n",
       "121      F\n",
       "122    NaN\n",
       "123      F\n",
       "124      M\n",
       "125      M\n",
       "126      M\n",
       "127      M\n",
       "128      M\n",
       "129      M\n",
       "130      M\n",
       "131      M\n",
       "132      M\n",
       "133    NaN\n",
       "134      F\n",
       "135      M\n",
       "136      M\n",
       "137      M\n",
       "138      M\n",
       "139      M\n",
       "140      M\n",
       "141    NaN\n",
       "142      M\n",
       "143      M\n",
       "144      M\n",
       "145    NaN\n",
       "146      M\n",
       "147      M\n",
       "148      M\n",
       "149      F\n",
       "150      M\n",
       "151      F\n",
       "152    NaN\n",
       "153      M\n",
       "154      M\n",
       "155      M\n",
       "156      F\n",
       "157      M\n",
       "158      M\n",
       "159      M\n",
       "160      M\n",
       "161      M\n",
       "162      F\n",
       "163      M\n",
       "164      F\n",
       "165      M\n",
       "166      M\n",
       "167    NaN\n",
       "168      M\n",
       "169      M\n",
       "170      M\n",
       "171      M\n",
       "172      M\n",
       "173      M\n",
       "174      F\n",
       "175      M\n",
       "176      F\n",
       "177      M\n",
       "178      M\n",
       "179    NaN\n",
       "180      M\n",
       "181      M\n",
       "182      F\n",
       "183      M\n",
       "184    NaN\n",
       "185      M\n",
       "186      F\n",
       "187      M\n",
       "188      M\n",
       "189      F\n",
       "190      M\n",
       "191      M\n",
       "192      M\n",
       "193      M\n",
       "194      M\n",
       "195      M\n",
       "196      M\n",
       "197      M\n",
       "198      M\n",
       "199      F\n",
       "200      M\n",
       "201      M\n",
       "202      M\n",
       "203      M\n",
       "204      M\n",
       "205      M\n",
       "206    NaN\n",
       "207      M\n",
       "208      M\n",
       "209      M\n",
       "210      M\n",
       "211      M\n",
       "212      M\n",
       "213      M\n",
       "214      M\n",
       "215      M\n",
       "216      M\n",
       "217      M\n",
       "218      F\n",
       "219      M\n",
       "220      M\n",
       "221      F\n",
       "222      M\n",
       "223      M\n",
       "224      M\n",
       "225      M\n",
       "226      M\n",
       "227    NaN\n",
       "228      M\n",
       "229      M\n",
       "230      M\n",
       "231      M\n",
       "232      M\n",
       "233      M\n",
       "234      M\n",
       "235      F\n",
       "236      F\n",
       "237      M\n",
       "238      M\n",
       "239    NaN\n",
       "240      M\n",
       "241      M\n",
       "242      F\n",
       "243      M\n",
       "244      M\n",
       "245      M\n",
       "246      M\n",
       "247      M\n",
       "248      M\n",
       "249      M\n",
       "250      F\n",
       "251      M\n",
       "252      M\n",
       "253      M\n",
       "254    NaN\n",
       "255      M\n",
       "256      M\n",
       "257      M\n",
       "258      M\n",
       "259      M\n",
       "260    NaN\n",
       "261      M\n",
       "262      M\n",
       "263      M\n",
       "264      M\n",
       "265    NaN\n",
       "266      M\n",
       "267    NaN\n",
       "268      M\n",
       "269    NaN\n",
       "270      M\n",
       "271      M\n",
       "272      M\n",
       "273      F\n",
       "274    NaN\n",
       "275      M\n",
       "277      M\n",
       "278      M\n",
       "279      M\n",
       "280      M\n",
       "281      M\n",
       "282    NaN\n",
       "283    NaN\n",
       "284      M\n",
       "285    NaN\n",
       "286      M\n",
       "287      M\n",
       "288    NaN\n",
       "289    NaN\n",
       "290      M\n",
       "291      F\n",
       "292      M\n",
       "293      M\n",
       "294      M\n",
       "295      M\n",
       "296    NaN\n",
       "297      M\n",
       "298      M\n",
       "299      M\n",
       "300      M\n",
       "301      M\n",
       "302      M\n",
       "303      F\n",
       "304    NaN\n",
       "305      M\n",
       "306      M\n",
       "307      M\n",
       "308      M\n",
       "309      M\n",
       "310      M\n",
       "311    NaN\n",
       "312    NaN\n",
       "314      M\n",
       "315    NaN\n",
       "316      F\n",
       "317      M\n",
       "318      M\n",
       "319      M\n",
       "320      F\n",
       "321    NaN\n",
       "322      M\n",
       "323      M\n",
       "324      M\n",
       "325      M\n",
       "326      M\n",
       "327      F\n",
       "328    NaN\n",
       "329      M\n",
       "330      M\n",
       "331      M\n",
       "332    NaN\n",
       "333      M\n",
       "334    NaN\n",
       "335      M\n",
       "336      M\n",
       "337    NaN\n",
       "338      F\n",
       "339      M\n",
       "340      M\n",
       "341      M\n",
       "342      M\n",
       "343      M\n",
       "344      M\n",
       "345    NaN\n",
       "346      M\n",
       "347      M\n",
       "348      M\n",
       "349    NaN\n",
       "350      M\n",
       "351      M\n",
       "352    NaN\n",
       "353      F\n",
       "354      M\n",
       "355      M\n",
       "356      M\n",
       "357      M\n",
       "358      M\n",
       "359      M\n",
       "360      M\n",
       "361    NaN\n",
       "362      M\n",
       "363    NaN\n",
       "364      M\n",
       "365      F\n",
       "366      M\n",
       "367      M\n",
       "368      M\n",
       "369    NaN\n",
       "370      M\n",
       "371    NaN\n",
       "372      M\n",
       "373      M\n",
       "374      M\n",
       "375    NaN\n",
       "376      M\n",
       "377      M\n",
       "378      F\n",
       "379      M\n",
       "380      M\n",
       "381    NaN\n",
       "382      M\n",
       "383      M\n",
       "384      M\n",
       "385      M\n",
       "386      M\n",
       "387      M\n",
       "388      M\n",
       "389      M\n",
       "390    NaN\n",
       "391    NaN\n",
       "392    NaN\n",
       "393      M\n",
       "394      M\n",
       "395      M\n",
       "396      M\n",
       "397      M\n",
       "398      M\n",
       "399      M\n",
       "400      M\n",
       "401    NaN\n",
       "402      M\n",
       "403      M\n",
       "404    NaN\n",
       "405      F\n",
       "406      F\n",
       "407      M\n",
       "408      M\n",
       "409      M\n",
       "410    NaN\n",
       "411      M\n",
       "412      M\n",
       "413      M\n",
       "414      M\n",
       "415      M\n",
       "416      M\n",
       "417      M\n",
       "418      M\n",
       "419    NaN\n",
       "420      M\n",
       "422      M\n",
       "423      M\n",
       "424      M\n",
       "425      M\n",
       "426      M\n",
       "427    NaN\n",
       "428      M\n",
       "429      F\n",
       "430      M\n",
       "431      M\n",
       "432      M\n",
       "433    NaN\n",
       "434      M\n",
       "435      M\n",
       "436    NaN\n",
       "437      M\n",
       "438      M\n",
       "439      M\n",
       "440      M\n",
       "441      M\n",
       "442      M\n",
       "443      M\n",
       "444      M\n",
       "445    NaN\n",
       "446      M\n",
       "447      F\n",
       "448      M\n",
       "449      M\n",
       "450      M\n",
       "451      M\n",
       "452      M\n",
       "453      F\n",
       "454    NaN\n",
       "455      M\n",
       "456      M\n",
       "457      M\n",
       "458      M\n",
       "459      M\n",
       "460      M\n",
       "461      F\n",
       "463    NaN\n",
       "464    NaN\n",
       "465      M\n",
       "466      M\n",
       "467      M\n",
       "468      M\n",
       "469      M\n",
       "470      M\n",
       "471      M\n",
       "472      M\n",
       "473      M\n",
       "474      M\n",
       "475    NaN\n",
       "476      M\n",
       "477      M\n",
       "478      M\n",
       "479      M\n",
       "480      M\n",
       "481      M\n",
       "482      M\n",
       "483      M\n",
       "484      M\n",
       "485      M\n",
       "486      M\n",
       "487      F\n",
       "488      M\n",
       "489      M\n",
       "490      M\n",
       "491      M\n",
       "492      M\n",
       "493    NaN\n",
       "494      M\n",
       "495    NaN\n",
       "496      M\n",
       "497    NaN\n",
       "498      M\n",
       "499    NaN\n",
       "500      M\n",
       "501      M\n",
       "502    NaN\n",
       "503      M\n",
       "504    NaN\n",
       "505      F\n",
       "506      M\n",
       "507      M\n",
       "508      M\n",
       "509      M\n",
       "510      M\n",
       "511      M\n",
       "512      M\n",
       "513      M\n",
       "514      M\n",
       "515      M\n",
       "516      M\n",
       "517      M\n",
       "518      M\n",
       "519    NaN\n",
       "520      M\n",
       "521      M\n",
       "522      M\n",
       "523      M\n",
       "524      M\n",
       "525      M\n",
       "526      M\n",
       "527      M\n",
       "529      M\n",
       "530      M\n",
       "531      M\n",
       "532      M\n",
       "533    NaN\n",
       "534      M\n",
       "535      M\n",
       "536      M\n",
       "537      M\n",
       "538      M\n",
       "539      M\n",
       "540      F\n",
       "541      M\n",
       "542      M\n",
       "543      M\n",
       "544      M\n",
       "545      M\n",
       "546    NaN\n",
       "547      M\n",
       "548    NaN\n",
       "549    NaN\n",
       "550      M\n",
       "551      M\n",
       "552    NaN\n",
       "553      M\n",
       "554      M\n",
       "555      M\n",
       "556    NaN\n",
       "557      M\n",
       "558      M\n",
       "559      F\n",
       "560      M\n",
       "Name: Sex, dtype: object"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex\n",
       "M      411\n",
       "NaN     80\n",
       "F       64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NaN , F, M\n",
    "df['Sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b595985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Sex'] = df['Sex'].replace({\n",
    "    'm' : 'M', \n",
    "    'f' : 'F', \n",
    "    'Male' : 'M', \n",
    "    'Female' : 'F'})\n",
    "\n",
    "df['Sex'] = df['Sex'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8e22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex\n",
       "M    411\n",
       "F     64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.dropna (subset = ['Sex'], inplace= True)\n",
    "df['Sex'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c9ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Year', 'Type', 'Country', 'State', 'Location', 'Activity',\n",
       "       'Name', 'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species ',\n",
       "       'Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Year',\n",
       " 'Type',\n",
       " 'Country',\n",
       " 'State',\n",
       " 'Location',\n",
       " 'Activity',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'Injury',\n",
       " 'Fatal Y/N',\n",
       " 'Time',\n",
       " 'Species ',\n",
       " 'Source']"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371e788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475, 15)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faa19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     475\n",
       "unique      2\n",
       "top         M\n",
       "freq      411\n",
       "Name: Sex, dtype: object"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save COUNTRY CLEANED file\n",
    "df.to_csv(\"../data/interim/2_interim_sex.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812493ff",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Age' CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fbf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 475 entries, 0 to 560\n",
      "Series name: Age\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "233 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df['Age'].info()\n",
    "#238 non-null object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['44', '20/30', '73', '6', '32', '50', '11', '26', nan, '35', '62',\n",
       "       '34', '31', '33', '25', '10', '8', '18', '68', '20', '19', '27',\n",
       "       '60', 'Teen', '65', '9', '40', '39', '43', '23', '38', '63', '47',\n",
       "       '48', '42', '24', '12', '16', '14', '7', '49', '17', '52', '53',\n",
       "       '45', '36', '54', '51', '22', '28', '56', '8 or 10', '75',\n",
       "       '23 & 20', '37', '29', '21', '15', '30', '16 to 18', '67', '77',\n",
       "       'mid-20s', 'Ca. 33', '? & 19', '46', '37, 67, 35, 27,  ? & 27',\n",
       "       '21, 34,24 & 35', '34 & 19', '13', '5', '1'], dtype=object)"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e197f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425900bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    44.0\n",
      "1    25.0\n",
      "2    73.0\n",
      "3     6.0\n",
      "4    32.0\n",
      "Name: Age, dtype: float64\n",
      "Median age used: 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sboub\\AppData\\Local\\Temp\\ipykernel_18192\\1912699362.py:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age_clean'].fillna(median_age, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "def clean_age(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove common unwanted characters\n",
    "    value = value.replace(\"years\", \"\").replace(\"ca.\", \"\").replace(\"about\", \"\").strip()\n",
    "\n",
    "    # Words that indicate no age\n",
    "    if value in [\"n/a\", \"na\", \"none\", \"?\", \"\", \"unknown\"]:\n",
    "        return np.nan\n",
    "\n",
    "    # Teen keyword (assume average age 15)\n",
    "    if \"teen\" in value:\n",
    "        return 15\n",
    "\n",
    "    # mid-20s -> 25\n",
    "    if \"mid\" in value and \"20\" in value:\n",
    "        return 25\n",
    "\n",
    "    # Capture numbers if present\n",
    "    nums = re.findall(r\"\\d+\", value)\n",
    "\n",
    "    if len(nums) == 1:\n",
    "        return int(nums[0])\n",
    "    \n",
    "    # If multiple numbers like \"20/30\", \"24 & 35\", \"16 to 18\", take the average\n",
    "    if len(nums) >= 2:\n",
    "        nums = [int(n) for n in nums]\n",
    "        return sum(nums) / len(nums)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Apply cleaning function\n",
    "df['Age_clean'] = df['Age'].apply(clean_age)\n",
    "\n",
    "# Convert to numeric properly\n",
    "df['Age_clean'] = pd.to_numeric(df['Age_clean'], errors='coerce')\n",
    "\n",
    "# Fill missing values with median\n",
    "median_age = df['Age_clean'].median()\n",
    "df['Age_clean'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Replace the values in 'Age' with those in 'Age_clean'\n",
    "df['Age'] = df['Age_clean']\n",
    "\n",
    "# Drop the 'Age_clean' column\n",
    "df.drop(columns=['Age_clean'], inplace=True)\n",
    "\n",
    "# Confirm results\n",
    "print(df['Age'].head(5))\n",
    "print(\"Median age used:\", median_age)\n",
    "\n",
    "# (Optional) Save cleaned file\n",
    "df.to_csv(r\"C:\\Users\\sboub\\Documents\\GitHub\\02_project-shark-attacks\\data\\clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acbd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    561\n",
       "mean      27\n",
       "std       10\n",
       "min        1\n",
       "25%       26\n",
       "50%       26\n",
       "75%       26\n",
       "max       77\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].describe().round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age  Age_clean\n",
      "0      44         44\n",
      "1   20/30         25\n",
      "2      73         73\n",
      "3       6          6\n",
      "4      32         32\n",
      "5      50         50\n",
      "6      11         11\n",
      "7      26         26\n",
      "8     NaN         26\n",
      "9     NaN         26\n",
      "10    NaN         26\n",
      "11    NaN         26\n",
      "12     35         35\n",
      "13    NaN         26\n",
      "14    NaN         26\n",
      "15     50         50\n",
      "16    NaN         26\n",
      "17     62         62\n",
      "18     34         34\n",
      "19     31         31\n",
      "Median age used: 26.0\n",
      "\n",
      " Age\n",
      "26    328\n",
      "17     15\n",
      "20      9\n",
      "43      8\n",
      "25      8\n",
      "24      8\n",
      "19      7\n",
      "18      7\n",
      "32      7\n",
      "22      7\n",
      "21      7\n",
      "27      6\n",
      "23      6\n",
      "11      6\n",
      "31      6\n",
      "36      6\n",
      "28      5\n",
      "9       5\n",
      "40      5\n",
      "35      5\n",
      "10      5\n",
      "14      5\n",
      "49      4\n",
      "44      4\n",
      "39      4\n",
      "52      4\n",
      "45      4\n",
      "12      4\n",
      "34      3\n",
      "29      3\n",
      "30      3\n",
      "50      3\n",
      "47      3\n",
      "37      3\n",
      "7       3\n",
      "53      3\n",
      "42      3\n",
      "33      3\n",
      "15      3\n",
      "68      2\n",
      "38      2\n",
      "8       2\n",
      "6       2\n",
      "62      2\n",
      "60      2\n",
      "13      2\n",
      "63      2\n",
      "48      2\n",
      "16      2\n",
      "51      2\n",
      "73      1\n",
      "65      1\n",
      "54      1\n",
      "75      1\n",
      "56      1\n",
      "77      1\n",
      "67      1\n",
      "46      1\n",
      "2       1\n",
      "5       1\n",
      "1       1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sboub\\AppData\\Local\\Temp\\ipykernel_18192\\2878044565.py:51: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age_clean'].fillna(median_age, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# DATA .csv\n",
    "df = pd.read_csv(url)\n",
    "# def Function : clean_age(value=age)\n",
    "def clean_age(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    value = str(value).strip().lower()\n",
    "\n",
    "    # Remove useless text\n",
    "    value = value.replace(\"years\", \"\").replace(\"year\", \"\").replace(\"ca.\", \"\").replace(\"about\", \"\").strip()\n",
    "\n",
    "    # Words meaning no age\n",
    "    if value in [\"n/a\", \"na\", \"none\", \"?\", \"\", \"unknown\"]:\n",
    "        return np.nan\n",
    "\n",
    "    # Teen keyword (approx.)\n",
    "    if \"teen\" in value:\n",
    "        return 15\n",
    "\n",
    "    # mid-20s ‚Üí 25\n",
    "    if \"mid\" in value and \"20\" in value:\n",
    "        return 25\n",
    "\n",
    "    # Extract numbers\n",
    "    nums = re.findall(r\"\\d+\", value)\n",
    "\n",
    "    # Single number\n",
    "    if len(nums) == 1:\n",
    "        return int(nums[0])\n",
    "    \n",
    "    # Multiple numbers -> average them (20/30, 23 & 20, 16 to 18)\n",
    "    if len(nums) >= 2:\n",
    "        nums = [int(n) for n in nums]\n",
    "        return sum(nums) / len(nums)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Apply cleaner\n",
    "df['Age_clean'] = df['Age'].apply(clean_age)\n",
    "\n",
    "# Convert to numeric\n",
    "df['Age_clean'] = pd.to_numeric(df['Age_clean'], errors='coerce')\n",
    "\n",
    "# Fill missing with median\n",
    "median_age = df['Age_clean'].median()\n",
    "df['Age_clean'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Convert to integer (no floats)\n",
    "df['Age_clean'] = df['Age_clean'].round().astype(int)\n",
    "\n",
    "# Show result\n",
    "print(df[['Age', 'Age_clean']].head(20))\n",
    "print(\"Median age used:\", median_age)\n",
    "\n",
    "df['Age'] = df['Age_clean']\n",
    "\n",
    "# Check result\n",
    "print(\"\\n\",df[\"Age\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d4faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    561.00\n",
       "mean      27.47\n",
       "std       10.18\n",
       "min        1.00\n",
       "25%       26.00\n",
       "50%       26.00\n",
       "75%       26.00\n",
       "max       77.00\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12343555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Percentile Statistics:\n",
      "Min:   1\n",
      "25%:   26\n",
      "50% (Median): 26\n",
      "75%:   26\n",
      "Max:   77\n"
     ]
    }
   ],
   "source": [
    "percentiles = df['Age'].describe(percentiles=[0.25, 0.5, 0.75])\n",
    "\n",
    "print(\"Age Percentile Statistics:\")\n",
    "print(f\"Min:   {int(percentiles['min'])}\")\n",
    "print(f\"25%:   {int(percentiles['25%'])}\")\n",
    "print(f\"50% (Median): {int(percentiles['50%'])}\")\n",
    "print(f\"75%:   {int(percentiles['75%'])}\")\n",
    "print(f\"Max:   {int(percentiles['max'])}\")\n",
    "\n",
    "# the majority were people up to 26 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3490f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    561\n",
       "mean      27\n",
       "std       10\n",
       "min        1\n",
       "25%       26\n",
       "50%       26\n",
       "75%       26\n",
       "max       77\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].describe().round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0588cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>Age_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25 Aug 2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21 Aug-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-Jun-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02-Mar-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Feb-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Year          Type     Country                   State  \\\n",
       "0  25 Aug 2023  2023.0    Unprovoked   AUSTRALIA         New South Wales   \n",
       "1  21 Aug-2023  2023.0  Questionable     BAHAMAS  New Providence   Isoad   \n",
       "2  07-Jun-2023  2023.0    Unprovoked     BAHAMAS                Freeport   \n",
       "3  02-Mar-2023  2023.0    Unprovoked  SEYCHELLES          Praslin Island   \n",
       "4  18-Feb-2023  2023.0  Questionable   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex  Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M   44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M   25   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F   73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M    6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M   32   \n",
       "\n",
       "                                              Injury  Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs        NaN      10h00   \n",
       "1  Body found with shark bites. Possible drowning...        NaN    Morning   \n",
       "2                               Calf severely bitten        NaN      13h00   \n",
       "3                                   Left foot bitten        NaN  Afternoon   \n",
       "4                              Death by misadventure        NaN        NaN   \n",
       "\n",
       "                Species                            Source  Age_clean  \n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF         44  \n",
       "1                    NaN           The Tribune, 8/21/2023         25  \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF         73  \n",
       "3            Lemon shark              Midlibre, 3/18/2023          6  \n",
       "4                    NaN              El Pais,  2/27/2023         32  "
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c3506",
   "metadata": {},
   "source": [
    "-----\n",
    "### 'Type' Injury & 'Fatal Y/N'\n",
    "**Fatality depends on Type of injury and severity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc3166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Fatal Y/N'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury Type:\n",
      "Unknown: 399 cases  (71.12%)\n",
      "No injury: 87 cases  (15.51%)\n",
      "Missing / Unknown: 38 cases  (6.77%)\n",
      "Not a shark: 27 cases  (4.81%)\n",
      "Provoked incident: 5 cases  (0.89%)\n",
      "Hoax / False report: 5 cases  (0.89%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def classify_injury(injury, fatal):\n",
    "    if pd.isna(injury):\n",
    "        injury = \"\"\n",
    "    injury = injury.lower()\n",
    "    fatal = str(fatal).strip().upper()\n",
    "\n",
    "    # HOAX / FALSE\n",
    "    if \"hoax\" in injury or \"false\" in injury:\n",
    "        return \"Hoax / False report\"\n",
    "\n",
    "    # NOT A SHARK\n",
    "    if any(x in injury for x in [\"not a shark\", \"stingray\", \"barracuda\", \"propeller\", \"fish\"]):\n",
    "        return \"Not a shark\"\n",
    "\n",
    "    # NO INJURY\n",
    "    if \"no injury\" in injury or \"no injuries\" in injury:\n",
    "        return \"No injury\"\n",
    "\n",
    "    # PROVOKED\n",
    "    if \"provoked\" in injury:\n",
    "        return \"Provoked incident\"\n",
    "\n",
    "    # MISSING\n",
    "    if any(x in injury for x in [\"missing\", \"disappeared\", \"body not recovered\"]):\n",
    "        return \"Missing / Unknown\"\n",
    "\n",
    "    # FATAL CASES\n",
    "    if fatal == \"Y\":\n",
    "        # drowned then bitten after death\n",
    "        if \"post-mortem\" in injury or (\"drown\" in injury and \"post\" in injury):\n",
    "            return \"Fatal | Drowned, shark scavenged\"\n",
    "        if \"unconfirmed\" in injury or \"probable\" in injury:\n",
    "            return \"Fatal | Unconfirmed shark involvement\"\n",
    "        return \"Fatal | Shark confirmed\"\n",
    "\n",
    "    # NON-FATAL CASES\n",
    "    if fatal == \"N\":\n",
    "        if any(x in injury for x in [\"bitten\", \"lacerat\", \"puncture\", \"wound\", \"abrasion\"]):\n",
    "            return \"Non-fatal | Confirmed shark bite\"\n",
    "        # Text but no bite\n",
    "        return \"Non-fatal | Other\"\n",
    "\n",
    "    # Unknown\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"Type\"] = df.apply(lambda row: classify_injury(row[\"Injury\"], row[\"Fatal Y/N\"]), axis=1)\n",
    "\n",
    "\n",
    "# Check RESULTS--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Count number of records in each injury type\n",
    "counts = df[\"Type\"].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = (counts / len(df)) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    \"Cases\": counts,\n",
    "    \"Percentage\": percentages.round(2)\n",
    "})\n",
    "\n",
    "\n",
    "# Save TYPE of INJURY CLEANED file\n",
    "df.to_csv(\"../data/interim/4_interim_type_injury.csv\", index=False)\n",
    "\n",
    "# Print Injury type\n",
    "print(\"Injury Type:\")\n",
    "for category in counts.index:\n",
    "    print(f\"{category}: {counts[category]} cases  ({percentages[category]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3514a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>Age_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25 Aug 2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21 Aug-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-Jun-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02-Mar-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Feb-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Year     Type     Country                   State  \\\n",
       "0  25 Aug 2023  2023.0  Unknown   AUSTRALIA         New South Wales   \n",
       "1  21 Aug-2023  2023.0  Unknown     BAHAMAS  New Providence   Isoad   \n",
       "2  07-Jun-2023  2023.0  Unknown     BAHAMAS                Freeport   \n",
       "3  02-Mar-2023  2023.0  Unknown  SEYCHELLES          Praslin Island   \n",
       "4  18-Feb-2023  2023.0  Unknown   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex  Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M   44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M   25   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F   73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M    6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M   32   \n",
       "\n",
       "                                              Injury  Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs        NaN      10h00   \n",
       "1  Body found with shark bites. Possible drowning...        NaN    Morning   \n",
       "2                               Calf severely bitten        NaN      13h00   \n",
       "3                                   Left foot bitten        NaN  Afternoon   \n",
       "4                              Death by misadventure        NaN        NaN   \n",
       "\n",
       "                Species                            Source  Age_clean  \n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF         44  \n",
       "1                    NaN           The Tribune, 8/21/2023         25  \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF         73  \n",
       "3            Lemon shark              Midlibre, 3/18/2023          6  \n",
       "4                    NaN              El Pais,  2/27/2023         32  "
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c2796",
   "metadata": {},
   "source": [
    "-----\n",
    "#### CLASSIFY INJURY SEVERITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         561\n",
       "unique          6\n",
       "top       Unknown\n",
       "freq          399\n",
       "Name: Type, dtype: object"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca64eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Injury Injury Severity Fatal Y/N\n",
      "0                     Severe injuries to lower limbs   Other/Unknown         N\n",
      "1  Body found with shark bites. Possible drowning...           Death         Y\n",
      "2                               Calf severely bitten   Severe injury         N\n",
      "3                                   Left foot bitten    Minor injury         N\n",
      "4                              Death by misadventure           Death         Y\n",
      "5                Right forearm and left hand injured   Other/Unknown         N\n",
      "6                           Minor cuts to left thigh    Minor injury         N\n",
      "7                                        Disappeared   Other/Unknown         N\n",
      "8           Laceration to arm caused by metal object    Minor injury         N\n",
      "9  No injury to occupants, injury to shark attemp...       No injury         N\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to classify injury severity\n",
    "def classify_severity(val):\n",
    "    if pd.isna(val):\n",
    "        val = \"\"\n",
    "    val = val.lower()\n",
    "    \n",
    "    if re.search(r'fatal|death|died|deceased', val):\n",
    "        return \"Death\"\n",
    "    if re.search(r'amputation|severed|severely bitten', val):\n",
    "        return \"Severe injury\"\n",
    "    if re.search(r'bitten|bite|laceration|cuts|fracture', val):\n",
    "        return \"Minor injury\"\n",
    "    if re.search(r'no injury|not injured|unharmed', val):\n",
    "        return \"No injury\"\n",
    "    if re.search(r'possible drowning|unknown', val):\n",
    "        return \"Uncertain\"\n",
    "    \n",
    "    return \"Other/Unknown\"\n",
    "\n",
    "# Apply to create Injury Severity column\n",
    "df[\"Injury Severity\"] = df[\"Injury\"].apply(classify_severity)\n",
    "\n",
    "# Now derive Fatal Y/N based on Injury Severity\n",
    "def derive_fatal(severity):\n",
    "    if severity == \"Death\":\n",
    "        return \"Y\"\n",
    "    elif severity in [\"Severe injury\", \"Injury\", \"No injury\", \"Uncertain\", \"Other/Unknown\"]:\n",
    "        return \"N\"\n",
    "    return \"N\"\n",
    "\n",
    "df[\"Fatal Y/N\"] = df[\"Injury Severity\"].apply(derive_fatal)\n",
    "\n",
    "\n",
    "# Save INJURY SEVERITY CLEANED file\n",
    "df.to_csv(\"../data/interim/5_interim_injury_severity.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Check the first rows\n",
    "print(df[[\"Injury\", \"Injury Severity\", \"Fatal Y/N\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c52fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     561\n",
       "unique      2\n",
       "top         N\n",
       "freq      450\n",
       "Name: Fatal Y/N, dtype: object"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save INJURY SEVERITY CLEANED file\n",
    "df.to_csv(\"../data/interim/6_interim.fatality.csv\", index=False)\n",
    "\n",
    "df['Fatal Y/N'].describe()\n",
    "# majority survive shark attacks !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedd299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>Age_clean</th>\n",
       "      <th>Injury Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25 Aug 2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>N</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>44</td>\n",
       "      <td>Other/Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21 Aug-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>25</td>\n",
       "      <td>Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-Jun-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>N</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>73</td>\n",
       "      <td>Severe injury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02-Mar-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>N</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>6</td>\n",
       "      <td>Minor injury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-Feb-2023</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>32</td>\n",
       "      <td>Death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Year     Type     Country                   State  \\\n",
       "0  25 Aug 2023  2023.0  Unknown   AUSTRALIA         New South Wales   \n",
       "1  21 Aug-2023  2023.0  Unknown     BAHAMAS  New Providence   Isoad   \n",
       "2  07-Jun-2023  2023.0  Unknown     BAHAMAS                Freeport   \n",
       "3  02-Mar-2023  2023.0  Unknown  SEYCHELLES          Praslin Island   \n",
       "4  18-Feb-2023  2023.0  Unknown   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex  Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M   44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M   25   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F   73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M    6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M   32   \n",
       "\n",
       "                                              Injury Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs         N      10h00   \n",
       "1  Body found with shark bites. Possible drowning...         Y    Morning   \n",
       "2                               Calf severely bitten         N      13h00   \n",
       "3                                   Left foot bitten         N  Afternoon   \n",
       "4                              Death by misadventure         Y        NaN   \n",
       "\n",
       "                Species                            Source  Age_clean  \\\n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF         44   \n",
       "1                    NaN           The Tribune, 8/21/2023         25   \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF         73   \n",
       "3            Lemon shark              Midlibre, 3/18/2023          6   \n",
       "4                    NaN              El Pais,  2/27/2023         32   \n",
       "\n",
       "  Injury Severity  \n",
       "0   Other/Unknown  \n",
       "1           Death  \n",
       "2   Severe injury  \n",
       "3    Minor injury  \n",
       "4           Death  "
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7cb8d",
   "metadata": {},
   "source": [
    "----------\n",
    "### 'Date' CLEANING AND SEPERATE INTO DD MM YYYY COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f12ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>DD</th>\n",
       "      <th>MM</th>\n",
       "      <th>YYYY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>25</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>20/30</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>07</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>02</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>18</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year          Type     Country                   State  \\\n",
       "0  2023.0    Unprovoked   AUSTRALIA         New South Wales   \n",
       "1  2023.0  Questionable     BAHAMAS  New Providence   Isoad   \n",
       "2  2023.0    Unprovoked     BAHAMAS                Freeport   \n",
       "3  2023.0    Unprovoked  SEYCHELLES          Praslin Island   \n",
       "4  2023.0  Questionable   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex    Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M     44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M  20/30   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F     73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M      6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M     32   \n",
       "\n",
       "                                              Injury  Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs        NaN      10h00   \n",
       "1  Body found with shark bites. Possible drowning...        NaN    Morning   \n",
       "2                               Calf severely bitten        NaN      13h00   \n",
       "3                                   Left foot bitten        NaN  Afternoon   \n",
       "4                              Death by misadventure        NaN        NaN   \n",
       "\n",
       "                Species                            Source  DD   MM  YYYY  \n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF  25  Aug  2023  \n",
       "1                    NaN           The Tribune, 8/21/2023  21  Aug  2023  \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF  07  Jun  2023  \n",
       "3            Lemon shark              Midlibre, 3/18/2023  02  Mar  2023  \n",
       "4                    NaN              El Pais,  2/27/2023  18  Feb  2023  "
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# DataFrame with the 'Date' column\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Function to parse dates-------------------------------------------------------------------------------------------------------------------\n",
    "def parse_date(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Remove 'Reported ' prefix\n",
    "    date_str = re.sub(r'^Reported\\s+', '', date_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Patterns to match\n",
    "    patterns = [\n",
    "        r'(?P<DD>\\d{1,2})[- ](?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',  # DD MMM YYYY\n",
    "        r'(?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',                      # MMM-YYYY\n",
    "        r'(?P<YYYY>\\d{4})',                                           # YYYY only\n",
    "    ]\n",
    "    \n",
    "    for pat in patterns:\n",
    "        match = re.match(pat, date_str)\n",
    "        if match:\n",
    "            return match.groupdict()\n",
    "    \n",
    "    return {'DD': None, 'MM': None, 'YYYY': None}\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Call Function to the column\n",
    "parsed_dates = df['Date'].apply(parse_date).apply(pd.Series)\n",
    "\n",
    "# Merge back to original DataFrame\n",
    "df = pd.concat([df, parsed_dates], axis=1)\n",
    "\n",
    "# drop the column 'Date' and add the new columns DD MM YYYY\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "#\n",
    "df = df[df[\"MM\"] != \"Early\"]\n",
    "df = df[df[\"MM\"] != \"Circa\"]\n",
    "df = df[df[\"MM\"] != \"Late\"]\n",
    "df = df[df[\"MM\"] != \"Before\"]\n",
    "\n",
    "# save added columns to our DataFrame\n",
    "df.to_csv(\"../data/interim/7_interim_date.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048e227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aug', 'Jun', 'Mar', 'Feb', 'Nov', 'Oct', 'Sep', 'Jul', 'Dec',\n",
       "       'Apr', 'May', 'Jan'], dtype=object)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# Clean the MM from 'NaN' and 'None'\n",
    "df = df[df[\"MM\"].notna() & (df[\"MM\"] != \"\")]\n",
    "\n",
    "# Replace full month names with abbreviations\n",
    "df[\"MM\"] = df[\"MM\"].replace({\n",
    "    \"November\": \"Nov\",\n",
    "    \"April\": \"Apr\"\n",
    "})\n",
    "\n",
    "df['MM'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1580ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# DATA .csv\n",
    "df = pd.read_csv(url)\n",
    "# Function to parse dates-------------------------------------------------------------------------------------------------------------------\n",
    "def parse_date(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Remove 'Reported ' prefix\n",
    "    date_str = re.sub(r'^Reported\\s+', '', date_str, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Patterns to match\n",
    "    patterns = [\n",
    "        r'(?P<DD>\\d{1,2})[- ](?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',  # DD MMM YYYY\n",
    "        r'(?P<MM>[A-Za-z]+)[- ](?P<YYYY>\\d{4})',                      # MMM-YYYY\n",
    "        r'(?P<YYYY>\\d{4})',                                           # YYYY only\n",
    "    ]\n",
    "    \n",
    "    for pat in patterns:\n",
    "        match = re.match(pat, date_str)\n",
    "        if match:\n",
    "            return match.groupdict()\n",
    "    \n",
    "    return {'DD': None, 'MM': None, 'YYYY': None}\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Call Function to the column\n",
    "parsed_dates = df['Date'].apply(parse_date).apply(pd.Series)\n",
    "\n",
    "# Merge back to original DataFrame\n",
    "df = pd.concat([df, parsed_dates], axis=1)\n",
    "\n",
    "# drop the column 'Date' and add the new columns DD MM YYYY\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "#\n",
    "df = df[df[\"MM\"] != \"Early\"]\n",
    "df = df[df[\"MM\"] != \"Circa\"]\n",
    "df = df[df[\"MM\"] != \"Late\"]\n",
    "df = df[df[\"MM\"] != \"Before\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aug', 'Jun', 'Mar', 'Feb', 'Nov', 'Oct', 'Sep', 'Jul', 'Dec',\n",
       "       'Apr', 'May', 'Jan'], dtype=object)"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# Clean the MM from 'NaN' and 'None'\n",
    "df = df[df[\"MM\"].notna() & (df[\"MM\"] != \"\")]\n",
    "\n",
    "# Replace full month names with abbreviations\n",
    "df[\"MM\"] = df[\"MM\"].replace({\n",
    "    \"November\": \"Nov\",\n",
    "    \"April\": \"Apr\"\n",
    "})\n",
    "\n",
    "df['MM'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of months (MM_clean):\n",
      "MM_clean\n",
      "1     54\n",
      "2     25\n",
      "3     33\n",
      "4     48\n",
      "5     28\n",
      "6     45\n",
      "7     64\n",
      "8     72\n",
      "9     37\n",
      "10    32\n",
      "11    34\n",
      "12    39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage of each month (MM_clean):\n",
      "MM_clean\n",
      "1     10.57%\n",
      "2      4.89%\n",
      "3      6.46%\n",
      "4      9.39%\n",
      "5      5.48%\n",
      "6      8.81%\n",
      "7     12.52%\n",
      "8     14.09%\n",
      "9      7.24%\n",
      "10     6.26%\n",
      "11     6.65%\n",
      "12     7.63%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mapping month names to integers\n",
    "month_map = {\n",
    "    'Jan': 1, 'January': 1,\n",
    "    'Feb': 2, 'February': 2,\n",
    "    'Mar': 3, 'March': 3,\n",
    "    'Apr': 4, 'April': 4,\n",
    "    'May': 5,\n",
    "    'Jun': 6, 'June': 6,\n",
    "    'Jul': 7, 'July': 7,\n",
    "    'Aug': 8, 'August': 8,\n",
    "    'Sep': 9, 'Sept': 9, 'September': 9,\n",
    "    'Oct': 10, 'October': 10,\n",
    "    'Nov': 11, 'November': 11,\n",
    "    'Dec': 12, 'December': 12\n",
    "}\n",
    "\n",
    "# Map MM column to integers; non-months become NaN\n",
    "df['MM_clean'] = df['MM'].map(month_map)\n",
    "\n",
    "# Drop non-month values\n",
    "df_months_only = df.dropna(subset=['MM_clean']).copy()\n",
    "\n",
    "# Convert to int\n",
    "df_months_only['MM_clean'] = df_months_only['MM_clean'].astype(int)\n",
    "\n",
    "# Compute percentages\n",
    "mm_percent = df_months_only['MM_clean'].value_counts(normalize=True) * 100\n",
    "\n",
    "\n",
    "# Compute counts from the cleaned DataFrame\n",
    "mm_counts = df_months_only['MM_clean'].value_counts()\n",
    "\n",
    "# Display results\n",
    "print(\"Counts of months (MM_clean):\")\n",
    "print(mm_counts.sort_index())  # months in chronological order\n",
    "\n",
    "mm_percent = df_months_only['MM_clean'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Round to 2 decimal places and add % sign\n",
    "mm_percent_formatted = mm_percent.sort_index().map(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# Display\n",
    "print(\"\\nPercentage of each month (MM_clean):\")\n",
    "print(mm_percent_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3ce7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MM\n",
       "Aug    72\n",
       "Jul    64\n",
       "Jan    54\n",
       "Apr    48\n",
       "Jun    45\n",
       "Dec    39\n",
       "Sep    37\n",
       "Nov    34\n",
       "Mar    33\n",
       "Oct    32\n",
       "May    28\n",
       "Feb    25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of shark attack in every month\n",
    "df['MM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM\n",
      "Aug    14.09%\n",
      "Jul    12.52%\n",
      "Jan    10.57%\n",
      "Apr     9.39%\n",
      "Jun     8.81%\n",
      "Dec     7.63%\n",
      "Sep     7.24%\n",
      "Nov     6.65%\n",
      "Mar     6.46%\n",
      "Oct     6.26%\n",
      "May     5.48%\n",
      "Feb     4.89%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of shark attacks per month\n",
    "percentages = df['MM'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Sort from high to low\n",
    "percentages_sorted = percentages.sort_values(ascending=False)\n",
    "\n",
    "# Format as 00.00%\n",
    "percentages_formatted = percentages_sorted.map(\"{:.2f}%\".format)\n",
    "\n",
    "print(percentages_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd4783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>DD</th>\n",
       "      <th>MM</th>\n",
       "      <th>YYYY</th>\n",
       "      <th>MM_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>25</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>20/30</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>07</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>02</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>18</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year          Type     Country                   State  \\\n",
       "0  2023.0    Unprovoked   AUSTRALIA         New South Wales   \n",
       "1  2023.0  Questionable     BAHAMAS  New Providence   Isoad   \n",
       "2  2023.0    Unprovoked     BAHAMAS                Freeport   \n",
       "3  2023.0    Unprovoked  SEYCHELLES          Praslin Island   \n",
       "4  2023.0  Questionable   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex    Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M     44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M  20/30   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F     73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M      6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M     32   \n",
       "\n",
       "                                              Injury  Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs        NaN      10h00   \n",
       "1  Body found with shark bites. Possible drowning...        NaN    Morning   \n",
       "2                               Calf severely bitten        NaN      13h00   \n",
       "3                                   Left foot bitten        NaN  Afternoon   \n",
       "4                              Death by misadventure        NaN        NaN   \n",
       "\n",
       "                Species                            Source  DD   MM  YYYY  \\\n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF  25  Aug  2023   \n",
       "1                    NaN           The Tribune, 8/21/2023  21  Aug  2023   \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF  07  Jun  2023   \n",
       "3            Lemon shark              Midlibre, 3/18/2023  02  Mar  2023   \n",
       "4                    NaN              El Pais,  2/27/2023  18  Feb  2023   \n",
       "\n",
       "   MM_clean  \n",
       "0         8  \n",
       "1         8  \n",
       "2         6  \n",
       "3         3  \n",
       "4         2  "
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save DATE CLEANED file\n",
    "df.to_csv(\"../data/interim/7_interim_date.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103d4e7",
   "metadata": {},
   "source": [
    "----\n",
    "### VIEWING CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal Y/N</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Source</th>\n",
       "      <th>DD</th>\n",
       "      <th>MM</th>\n",
       "      <th>YYYY</th>\n",
       "      <th>MM_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lighthouse Beach, Port Macquarie</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>Toby Begg</td>\n",
       "      <td>M</td>\n",
       "      <td>44</td>\n",
       "      <td>Severe injuries to lower limbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10h00</td>\n",
       "      <td>White shark, 3.8-4.2m</td>\n",
       "      <td>B. Myatt, &amp; M. Michaelson, GSAF</td>\n",
       "      <td>25</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>New Providence   Isoad</td>\n",
       "      <td>Saunders Beach, Nassau</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>20/30</td>\n",
       "      <td>Body found with shark bites. Possible drowning...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Tribune, 8/21/2023</td>\n",
       "      <td>21</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>BAHAMAS</td>\n",
       "      <td>Freeport</td>\n",
       "      <td>Shark Junction</td>\n",
       "      <td>Scuba diving</td>\n",
       "      <td>Heidi Ernst</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>Calf severely bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13h00</td>\n",
       "      <td>Caribbean rreef shark</td>\n",
       "      <td>J. Marchand, GSAF</td>\n",
       "      <td>07</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SEYCHELLES</td>\n",
       "      <td>Praslin Island</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Snorkeling</td>\n",
       "      <td>Arthur ‚Ä¶</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>Left foot bitten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Lemon shark</td>\n",
       "      <td>Midlibre, 3/18/2023</td>\n",
       "      <td>02</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>ARGENTINA</td>\n",
       "      <td>Patagonia</td>\n",
       "      <td>Chubut Province</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diego Barr√≠a</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Death by misadventure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Pais,  2/27/2023</td>\n",
       "      <td>18</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year          Type     Country                   State  \\\n",
       "0  2023.0    Unprovoked   AUSTRALIA         New South Wales   \n",
       "1  2023.0  Questionable     BAHAMAS  New Providence   Isoad   \n",
       "2  2023.0    Unprovoked     BAHAMAS                Freeport   \n",
       "3  2023.0    Unprovoked  SEYCHELLES          Praslin Island   \n",
       "4  2023.0  Questionable   ARGENTINA               Patagonia   \n",
       "\n",
       "                            Location      Activity          Name Sex    Age  \\\n",
       "0  Lighthouse Beach, Port Macquarie        Surfing     Toby Begg   M     44   \n",
       "1             Saunders Beach, Nassau           NaN          male   M  20/30   \n",
       "2                     Shark Junction  Scuba diving   Heidi Ernst   F     73   \n",
       "3                                NaN    Snorkeling      Arthur ‚Ä¶   M      6   \n",
       "4                    Chubut Province           NaN  Diego Barr√≠a   M     32   \n",
       "\n",
       "                                              Injury  Fatal Y/N       Time  \\\n",
       "0                     Severe injuries to lower limbs        NaN      10h00   \n",
       "1  Body found with shark bites. Possible drowning...        NaN    Morning   \n",
       "2                               Calf severely bitten        NaN      13h00   \n",
       "3                                   Left foot bitten        NaN  Afternoon   \n",
       "4                              Death by misadventure        NaN        NaN   \n",
       "\n",
       "                Species                            Source  DD   MM  YYYY  \\\n",
       "0  White shark, 3.8-4.2m  B. Myatt, & M. Michaelson, GSAF  25  Aug  2023   \n",
       "1                    NaN           The Tribune, 8/21/2023  21  Aug  2023   \n",
       "2  Caribbean rreef shark                J. Marchand, GSAF  07  Jun  2023   \n",
       "3            Lemon shark              Midlibre, 3/18/2023  02  Mar  2023   \n",
       "4                    NaN              El Pais,  2/27/2023  18  Feb  2023   \n",
       "\n",
       "   MM_clean  \n",
       "0         8  \n",
       "1         8  \n",
       "2         6  \n",
       "3         3  \n",
       "4         2  "
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33a020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'Type', 'Country', 'State', 'Location', 'Activity', 'Name',\n",
       "       'Sex', 'Age', 'Injury', 'Fatal Y/N', 'Time', 'Species ', 'Source', 'DD',\n",
       "       'MM', 'YYYY', 'MM_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe74f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
